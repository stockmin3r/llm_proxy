<h1 align="center">LLM-PROXY</h1>

LLM Proxy allows you to load multiple instances of llamacpp and multiple different models
Currently questions may be sent to the model via Discord but soon also via HTTP

