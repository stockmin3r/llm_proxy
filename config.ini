; This is the configuration file for the LLM Proxy application.
; It contains settings related to channels, server, models, and timeouts.

[channels]
channels=general,panda,bots
; List of channels that the LLM Proxy application will listen to.

[server]
panda_port=8484
; The port number for the Panda server.

llm_port_start=8080
; The starting port number for the LLM server.

use_pipes=1
; Flag indicating whether to use pipes for communication.

[models]
model=openhermes-2.5-mistral-7b.Q4_K_M.gguf
; The name of the model to be used by the LLM Proxy application.

nr_model_instances=1
; The number of model instances to be created.

model_directory=/usr/src/ai/llamacpp
; The directory where the model files are located.

timeout=60
; The maximum time (in seconds) to spend generating tokens for a single question.
