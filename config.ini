[channels]
channels=general,panda,bots

[server]
panda_port=8484
llm_port_start=8080
use_pipes=1

[models]
model=openhermes-2.5-mistral-7b.Q4_K_M.gguf
nr_model_instances=1
model_directory=/usr/src/ai/llamacpp
# max time to spend generating tokens for a single question (in seconds)
timeout=60
